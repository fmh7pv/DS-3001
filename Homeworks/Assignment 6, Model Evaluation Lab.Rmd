---
title: "\Assignment 6: Model Evaluation Lab"
author: "Fadumo Hussein"
date: "10/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The most important part of any machine learning model(or any model, really) is understanding and defining the models weaknesses and/or vulnerabilities. 

To do so we are going to practice on a familiar dataset and use a method we just learned, kNN. For this lab use the Job Placement or Bank dataset.  

```{r}
library(tidyverse)
library(caret)
library(RColorBrewer)
library(ROCR)
library(MLmetrics)
library(mltools)
library(data.table)
library(mice)
```
Part 1. Develop a ML question using status as the target variable. In consideration of all the metrics we discussed what are a couple of key metrics that should be tracked given the question you are trying to solve?
## Question 
```{r}
#Can we build a model that predicts job salary . Things that should be tracked given the question are 

url <- "https://raw.githubusercontent.com/DG1606/CMS-R-2020/master/Placement_Data_Full_Class.csv"

job_data <- read_csv(url)

View(job_data)

```


Part 2. Create a kNN model that can answer your question, using all the 
appropriate prep methods we discussed.   
#Question 2 
```{r}
str(job_data)
table(job_data$degree_t)

job_data[,c(2,4,6,7,9,10,12,14)] <- lapply(job_data[,c(2,4,6,7,9,10,12,14)], as.factor)

str(job_data)

job_data <- job_data[complete.cases(job_data),]

md.pattern(job_data, rotate.names = TRUE)

str(job_data)

(salary_c <- scale(job_data$salary, center = TRUE, scale = FALSE))#center but not standardized

(salary_sc <- scale(job_data$salary, center = TRUE, scale = TRUE))#center and standardized 


abc <- names(select_if(job_data, is.numeric))

job_data[abc] <- lapply(job_data[abc], normalize)

str(job_data)

job_data_1h <- one_hot(as.data.table(job_data),cols = "auto",sparsifyNAs = TRUE,naCols = FALSE,dropCols = TRUE,dropUnusedLevels = TRUE)

str(job_data_1h)

#added this a predictor versus replacing the numeric version
(job_data_1h$salary_f <- cut(job_data_1h$salary,c(0,.12,1),labels = c(0,1)))#why the NA? If we want two segments we input three numbers, start, cut and stop values

View(job_data_1h$salary_f)
str(job_data_1h)

#So no let's check the prevalence 
(prevalence <- table(job_data_1h$salary_f)[[2]]/length(job_data_1h$salary_f))

table(job_data_1h$salary_f)

job_data_dt <- job_data_1h[,-c("salary")]

view(job_data_dt)


part_index_1 <- caret::createDataPartition(job_data_dt$salary_f,
                                           times=1,#number of splits
                                           p = 0.70,#percentage of split
                                           groups=1,
                                           list=FALSE)
View(part_index_1)
dim(job_data_dt)

train <- job_data_dt[part_index_1,]#index the 70%
tune_and_test <- job_data_dt[-part_index_1, ]#index everything but the %70

#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$salary_f,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

dim(train)
dim(tune)
dim(test)

table(train$salary_f)#check the prevalance
37/(37+63 )
table(test$salary_f)
7 /(7+13)
table(tune$salary_f)
8 /(14+8)

str(job_data_1h)

job_data_1h <-job_data_1h[,-24]
job_data_1h<- job_data_1h[complete.cases(job_data_1h), ]
view(job_data_1h)
job_data_1h$salary_f 


table(job_data_1h$salary_f )[2]
table(job_data_1h$salary_f )[2]/sum(table(job_data_1h$salary_f ))




# This means that at random, we have an 36.62% chance of correctly picking
# out a exp award state value. Let's see if kNN can do any better.

part_index_1 <- createDataPartition(job_data_1h$salary_f ,
                                           times=1,
                                           p = 0.80,
                                           groups=1,
                                           list=FALSE)
View(part_index_1)

train <- job_data_1h[part_index_1,]
str(train)

tune_and_test <- job_data_1h[-part_index_1, ]


#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$salary_f ,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

str(train)
dim(tune)
dim(test)

set.seed(1984)
job_data_9NN <-  knn(train = train,#<- training set cases
               test = tune,    #<- tune set cases
               cl = train$salary_f,#<- category for true classification
               k = 3,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE)# provides the output in probabilities 
?knn
# View the output.
str(job_data_9NN)
table(job_data_9NN)
table(tune$job_data)


job_data_9NN

View(as.tibble(job_data_9NN))
View(as.tibble(attr(job_data_9NN,"prob")))

```
Part 3. Evaluate the model using the metrics you identified in the first question. Make sure to calculate/reference the prevalence to provide a baseline for some of these measures. Summarize the output of the key metrics you established in part 1. 
```{r}
kNN_res = table(job_data_9NN,
                tune$salary_f)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

#Accuracy = TP+TN/(TP+TN+FP+FN)

(9+3)/(9+3+2)


kNN_res[row(kNN_res) == col(kNN_res)]#essentially the left to right diagonal 

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)

kNN_acc

```


Part 4.  Consider where miss-classification errors (via confusion matrix) are occurring, is there a pattern? If so discuss this pattern and why you think this is the case. 
```{r}
confusionMatrix(as.factor(job_data_9NN), as.factor(tune$salary_f), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#There seems to be a type a bit of a type two error compared to type one. 
```

Part 5. Based on your exploration in Part 4, change the threshold using the function provided in the in-class example, what differences do you see in the evaluation metrics? Speak specifically to the metrics that are best suited to address the question you are trying to answer from part 1. 
```{r}
str(job_data_9NN)

job_data_prob_1 <- tibble(attr(job_data_9NN, "prob"))

View(job_data_prob_1)

final_model <- tibble(k_prob=job_data_prob_1$`attr(job_data_9NN, "prob")`,pred=job_data_9NN,target=tune$salary_f)

View(final_model)

#Need to convert this to the likelihood to be in the poss class.
final_model$pos_prec <- ifelse(final_model$pred == 0, 1-final_model$k_prob, final_model$k_prob)

View(final_model)

#Needs to be a factor to be correctly  
final_model$target <- as.factor(final_model$target)

densityplot(final_model$pos_prec)

#confusionMatrix from Caret package
confusionMatrix(final_model$pred, final_model$target, positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=tune_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

str(final_model)

adjust_thres(final_model$pos_prec,.60,as.factor(final_model$target))


#The model does not improve by much. 

```

Part 6. Summarize your findings (a paragraph or two) speaking through your question, what does the evaluation outputs mean when answering the question you've proposed?
```{r}
#My initial question was a model could predict salary based on the information given. I built a kNN model that attempts to answer this question. The evaluation output in this model means that this model has an 85.71% of predicting salary. However the model did not improve when changing the threshold. This means that they're variables that too much influence that might be causing the model to not improve.
```

Submit a .Rmd file along with the data used or access to the data sources to the Collab site. You can work together with your groups but submit individually and generate your own R file. 

