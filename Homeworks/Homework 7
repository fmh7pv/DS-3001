---
title: "Assignment 7: Decision Tree Part 1"
author: "Brian Wright"
date: "December 7, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Congrats! You just graduated UVA's MSDS program and got a job working at a movie studio in Hollywood. 

Your boss is the head of the studio and wants to know if they can gain a competitive advantage by predicting new movies that might get high imdb scores (movie rating). 

You would like to be able to explain the model to the mere mortals 
but need a fairly robust and flexible approach so you've 
chosen to use decision trees to get started. 

In doing so, similar to  great data scientists of the past 
you remembered the excellent education provided 
to you at UVA in a undergrad data science course and have outline 
20ish steps that will need to be undertaken to complete this task. As always, you will need to make sure to #comment your work heavily. 


 Footnotes: 
-	You can add or combine steps if needed
-	Also, remember to try several methods during evaluation and always be 
mindful of how the model will be used in practice.
- Make sure all your variables are the correct type (factor, character,numeric, etc.)

## Libraries
```{r}
library(rio)
library(plyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
install.packages("rpart.plot")
library(rpart.plot)
install.packages("rattle")
library(rattle)
library(caret)
library(C50) #Need this to pass into caret 
library(mlbench)
```

```{r}
#1 Load the data and ensure the labels are correct. restate the question and comment on evaluation metrics that you might pay attention to.  
movie <- read.csv('movie_metadata.csv')
View(movie)
str(movie)

sum(is.na(movie))

#drop directors 
movie<- movie[, c(-2, -7, -10, -11,-12, -15, -17, -18)]
str(movie)

```

```{r}
#2 Ensure all the variables are classified correctly including the target variable and collapse factor variables as needed. 
str(movie)



quan.names <-c("num_critic_for_reviews", "duration","director_facebook_likes","actor_3_facebook_likes","actor_1_facebook_likes","gross","num_voted_users","cast_total_facebook_likes","facenumber_in_poster", "num_user_for_reviews", "budget", "title_year","actor_2_facebook_likes", "aspect_ratio", "movie_facebook_likes")
movie[,quan.names] <- lapply(movie[,quan.names], as.numeric)

qual.names <-c("color", "country", "language")
movie[,qual.names] <- lapply(movie[,qual.names], as.factor)

table(movie$country)
movie$country <- fct_collapse(movie$country, 
                "Western Hemisphere" =c("USA","Mexico", "Canada", "Bahamas", "Dominican Republic", "Panama"),
                "Eatern Hemisphere" = c("Argentina", "Aruba","Brazil", "Chile", "Colombia", "Peru", "Belgium", "Bulgaria", "Czech Republic", "Denmark", "Finland", "France", "Georgia", "Germany", "Greece", "Hungary", "Iceland", "Ireland", "Italy", "Netherlands", "Norway", "Poland", "Romania", "Slovakia", "Slovenia", "Spain", "Sweden", "Switzerland", "UK", "Afghanistan", "Cambodia", "China", "Hong Kong", "India", "Indonesia", "Iran", "Israel", "Japan", "Kyrgyzstan", "Pakistan", "Philippines", "South Korea", "Taiwan", "Thailand", "Turkey", "United Arab Emirates", "Russia", "Cameroon", "Egypt", "Kenya", "Libya", "Nigeria", "South Africa","Australia", "New Zealand"),
                Other = c("New Line", "Official site", "Soviet Union", "West Germany", ""))
table(movie$country)  
table(movie$language)
movie$language <- fct_collapse(movie$language, 
                              English = ("English"),
                              NonEnglish = c("Aboriginal", "Arabic", "Aramaic", "Bosnian", "Cantonese", "Chinese", "Czech", "Danish", "Dari", "Dutch", "Dzongkha", "Filipino", "French", "German","Greek", "Hebrew", "Hindi", "Hungarian", "Icelandic", "Indonesian", "Italian", "Japanese", "Kannada", "Kazakh", "Korean", "Mandarin", "Maya", "Mongolian", "Norwegian", "Panjabi", "Persian", "Polish", "Portuguese", "Romanian", "Russian", "Slovenian", "Spanish", "Swahili", "Swedish", "Tamil", "Telugu", "Thai", "Urdu", "Vietnamese", "Zulu"), 
                              Other = c("None" ,""))
view(movie$language)
table(movie$color)
movie$color <- fct_collapse(movie$color, 
                            Color =c("Color"),
                            "Black and White"=c("Black and White"),
                            Other =c(""))
table(movie$color)
table(movie$content_rating)
movie$content_rating <-fct_collapse(movie$content_rating, 
                                    G =c("G"),
                                    PG =c("PG"),
                                    "PG-13" =c("PG-13"),
                                    R=c("R"),
                                    Other =c("", "Approved", "M", "NC-17", "Not Rated", "Passed", "Unrated", "X", "TV-14", "TV-G", "TV-MA", "TV-PG", "TV-Y", "TV-Y7"))
table(movie$content_rating)
```

```{r}
#3 Check for missing variables and correct as needed. 

summary(movie)#show the location by variable

mice::md.pattern(movie)#provides information on the location of the NAs

movie <- movie[complete.cases(movie),]

md.pattern(movie)

str(movie)

```

```{r}
#4 Guess what, you don't need to standardize the data,because DTs don't require this to be done, they make local greedy decisions...keeps getting easier, go to the next step
```

```{r}
#5 Determine the baserate or prevalence for the classifier, what does this number mean? (Need to cut the target appropriately)  


summary(movie$imdb_score)

movie <- movie %>% mutate(imdb_score = case_when(
  imdb_score > 7.200  ~ "high", 
  imdb_score <=  7.200  ~ "low",
))
view(movie)
table(movie$imdb_score)

movie$imdb_score <-factor(movie$imdb_score) 

movie$imdb_score <-fct_collapse(movie$imdb_score,
                                high=c("high"),
                                low=c("low"))                         

#Prevalance 
883/(3801+883)
#18.85% percent of films have an imdb rating of high in the dataset 
```

```{r}
#6 Split your data into test, tune, and train. (80/10/10)
set.seed(1999)
part_index_1 <- caret::createDataPartition(movie$imdb_score,
                                           times=1,
                                           p = 0.80,
                                           groups=1,
                                           list=FALSE)

train <- movie[part_index_1, ]
tune_and_test <- movie[-part_index_1, ]
train
#create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$imdb_score,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]


dim(train)
dim(test) 
dim(tune)
```


```{r}
#7 Build your model using the training data, rpart2, and repeated cross validation as reviewed in class with the caret package.

#drop direc
features <- train[,-18]

View(features)
target <- train$imdb_score

str(features)
str(target)

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary) 

tree.grid <- expand.grid(maxdepth=c(5,7,9,11))

set.seed(1984)
movie_mdl <- train(x=features,
                y=target,
                method="rpart2",
                trControl=fitControl,
                metric="ROC")

```


```{r}
#8 View the results, comment on how the model performed and which variables appear to be contributing the most (variable importance)  

movie_mdl$results
varImp(movie_mdl)


# The variables num_voted_users, num_user_for_reviews, and duration variables appear to be contributing the most

```

```{r}
#9 Plot the output of the model to see the tree visually, using rpart.plot 
plot(movie_mdl)


rpart.plot(movie_mdl$finalModel, type=4,extra=101)


```

```{r}
#10 Use the tune set and the predict function with your model to the 
# predict the target variable, making sure to produce probabilities.
predictandCM<- function(model,data,modeltype,ref)
{
  #model using, data going into the model, and output type for predict function
  pred <-predict(model,data,type=modeltype)
  confusionMatrix(pred, reference=ref, positive = 'high')
}


predictandCM(movie_mdl,tune,"raw",tune$imdb_score)#compare to the alternative model


#Can also do this without the function
movie_pred_tune = predict(movie_mdl,tune,tune$imdb_score, type= "prob")#probabilities

movie_pred_tune_labels = predict(movie_mdl,tune,tune$imdb_score,type = "raw")#Labels

View(as_tibble(movie_pred_tune_labels))


```

```{r}
#11 Use the the confusion matrix function on your predictions to check a variety of metrics and comment on the metric that might be best for this type of analysis given your question.

movie_eval <- caret::confusionMatrix(movie_pred_tune_labels, 
                as.factor(tune$imdb_score), 
                dnn=c("Prediction", "Actual"),
                positive="high",
                mode = "everything")

movie_eval

# Balanced Accuracy might be best for this type of analysis when trying to figure out what flims will be rated over 7.2

```

```{r}
#12 With the percentages you generated in step 10,select several different threshold levels using the threshold function we created and interpret the results. What patterns do you notice, did the evaluation metrics change? 

plot(density(movie_pred_tune$high))

adjust_thres <- function(x, y, z) {
  #x=tune_outcome, y=threshold, z=tune_outcome
  thres <- as.factor(ifelse(x > y, "high","low"))
  confusionMatrix(thres, z, positive = "high", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(movie_pred_tune$high,y=.7,tune$imdb_score)

#The accuracy and specificty increased but the sensitivity did decrease 
```

```{r}
#13 Based on your understanding of the model and data adjust the hyper-parameter via the built in train control function in caret or build and try new features, does the model quality improve or not? If so how and why, if not, why not?

tree.grid <- expand.grid(maxdepth=c(2,4,6,12))

set.seed(1984)
movie_mdl_1 <- train(x=features,
                y=target,
                method="rpart2",#type of model uses maxdepth to select a model
                trControl=fitControl,#previously created
                tuneGrid=tree.grid,#expanded grid
                metric="ROC")#selected on of the metrics available from two variable summary.

rpart.plot(movie_mdl_1$finalModel, type=5,extra=101)

movie_mdl$results
movie_mdl_1$results

#The model does not improve, I think the model is over-fitting after increasing the type and tree grid. 

```

```{r}
#14 Once you are confident that your model is not improving, via changes implemented on the training set and evaluated on the the tune set, predict with the test set and report final evaluation of the model. Discuss the output in comparison with the previous evaluations.  

predictandCM(movie_mdl,tune,"raw",tune$imdb_score)#compare to the alternative model

predictandCM(movie_mdl_1,tune,"raw",tune$imdb_score)#looks like the orginal model is likely better


```

```{r}
#15 Summarize what you learned along the way and make recommendations on how this could be used moving forward, being careful not to over promise. 

#I learned that decision making trees are a type of machine learning method where data is continuously split data based on the previous asked questions. A reccomendation would be potentially removing the branches with low variable importance
```

```{r}
#16 What was the most interesting or hardest part of this process and what questions do you still have? 
#The most interesting part of this assignment was seeing how continuous variables can be assessed by using decision  making trees.

#I have no questions 
```


